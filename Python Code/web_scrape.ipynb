{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c04a38",
   "metadata": {},
   "source": [
    "# Pronosoft Football Data - Webscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5451",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf25f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc924ff5",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979111cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as req\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import datetime as dt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344550b-4442-47ec-b1ee-f4d704e7f353",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027e78-efef-479d-9cca-cc9f8292c9b8",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614b359e-b4e2-4cce-871c-88dfa3cd5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_data = '..\\Data'\n",
    "csv_name = folder_data + '\\pronosoft_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30b3d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bca03b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22a1f5",
   "metadata": {},
   "source": [
    "Parse Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d0e2ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REQUESTS ACCESS & PARSES PAGE\n",
    "\n",
    "def parse_page(url):\n",
    "    # Grabs the page\n",
    "    r = req(url)\n",
    "    page_html = r.read()\n",
    "    r.close()\n",
    "\n",
    "    # Parsing means to divide (a sentence) into grammatical parts and identify the parts and their relations to each other\n",
    "    page_soup = soup(page_html,\"html.parser\")  \n",
    "    \n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08a33d",
   "metadata": {},
   "source": [
    "Extract Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2a34d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TAKES IN VALUES FROM PRONOSOFT\n",
    "\n",
    "def get_info(page_soup, pick_date):\n",
    "    day_scrape = []\n",
    "    \n",
    "    leagues_soup = page_soup.find_all(\"div\", {\"class\": \"dev-desktop-league-comp\"})\n",
    "    \n",
    "    for league_soup in leagues_soup:\n",
    "        league = league_soup.find_all(\"h3\")[0].get_text()\n",
    "        matches_soup = league_soup.table.tbody.find_all(\"tr\")[1:]\n",
    "        \n",
    "        for match_soup in matches_soup:\n",
    "            info_match_soup = match_soup.find_all(\"td\")\n",
    "            info = [i.get_text() for i in info_match_soup]\n",
    "            \n",
    "            if check_info_complete(info):\n",
    "                info_clean = get_info_clean(info)\n",
    "                \n",
    "                match_scrape = [pick_date, league] + info_clean\n",
    "                day_scrape.append(match_scrape)\n",
    "                            \n",
    "    return day_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa19f8",
   "metadata": {},
   "source": [
    "Check Info Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a51394",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHECK INFO IS COMPLETE (Prob., odds and scores)\n",
    "\n",
    "def check_info_complete(info):\n",
    "    if (info[3][0] == '-' or info[3][-1] == '%') or (info[7][0] == '-' or info[7][-1] == '%') or (info[9] == 'Annulé') or (info[-1] == 'Reporté' or info[-1] == '?-?'):\n",
    "        complete = False\n",
    "    else:\n",
    "        complete = True\n",
    "    \n",
    "    return complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85cf0f",
   "metadata": {},
   "source": [
    "Clean Up Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a2304a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_info_clean(info):\n",
    "    info_clean = []\n",
    "    \n",
    "    # Match time\n",
    "    info_clean.append(info[0].replace('h', ':'))\n",
    "    \n",
    "    # Team names\n",
    "    info_clean.append(info[1].split('-')[0][:-1])\n",
    "    info_clean.append(info[1].split('-')[1][1:])\n",
    "    \n",
    "    # Seperate prob. & odds \n",
    "    info_clean.append(np.round(0.01 * int(info[2].split('%')[0]), 2))\n",
    "    info_clean.append(info[2].split('%')[1].replace(',', '.'))\n",
    "    info_clean.append(np.round(0.01 * int(info[3].split('%')[0]), 2))\n",
    "    info_clean.append(info[3].split('%')[1].replace(',', '.'))\n",
    "    info_clean.append(np.round(0.01 * int(info[4].split('%')[0]), 2))\n",
    "    info_clean.append(info[4].split('%')[1].replace(',', '.'))\n",
    "    prediction_team = info[5]\n",
    "    if prediction_team == '-':\n",
    "        prediction_team = None\n",
    "    info_clean.append(prediction_team)\n",
    "    info_clean.append(np.round(0.01 * int(info[6].split('%')[0]), 2))\n",
    "    info_clean.append(info[6].split('%')[1].replace(',', '.'))\n",
    "    info_clean.append(np.round(0.01 * int(info[7].split('%')[0]), 2))\n",
    "    info_clean.append(info[7].split('%')[1].replace(',', '.'))\n",
    "    prediction_uo = info[8]\n",
    "    if prediction_uo == '-':\n",
    "        prediction_uo = None\n",
    "    info_clean.append(prediction_uo)\n",
    "    \n",
    "    # Scores\n",
    "    info_clean.append(info[9].split('-')[0])\n",
    "    info_clean.append(info[9].split('-')[1])\n",
    "            \n",
    "    return info_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591cdd9",
   "metadata": {},
   "source": [
    "Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef75da4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE CSV\n",
    "# Under / Over is betting whether some of scores will be Under or Over 2.5\n",
    "\n",
    "def save_info_csv(day_scrape):\n",
    "    \n",
    "    field_names = [ \n",
    "                    \"date\",\n",
    "                    \"league\",\n",
    "                    \"time\",\n",
    "                    \"team_1_name\",\n",
    "                    \"team_2_name\",\n",
    "                    \"team_1_prob\",\n",
    "                    \"team_1_bet_odds\",\n",
    "                    \"nul_prob\",\n",
    "                    \"nul_bet_odds\",\n",
    "                    \"team_2_prob\",\n",
    "                    \"team_2_bet_odds\",\n",
    "                    \"prediction_team_pronosoft\",\n",
    "                    \"under_prob\",\n",
    "                    \"under_bet_odds\",\n",
    "                    \"over_prob\",\n",
    "                    \"over_bet_odds\",\n",
    "                    \"prediction_uo_pronosoft\",\n",
    "                    \"team_1_score\",\n",
    "                    \"team_2_score\"\n",
    "                  ]\n",
    "        \n",
    "#     Create csv with fields, if it doesn't exist\n",
    "    if not path.exists(csv_name):\n",
    "        df_day_scrape = pd.DataFrame(columns = field_names)\n",
    "        df_day_scrape.to_csv(csv_name, index=False)\n",
    "        \n",
    "    dates = np.unique(pd.read_csv(csv_name)['date'])\n",
    "    \n",
    "    # Check if new scrape is not empty, and not already in database\n",
    "    if len(day_scrape) != 0 and day_scrape[0][0] not in dates:\n",
    "        with open(csv_name, 'a', newline = '', encoding = 'utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for scrape in day_scrape:\n",
    "                writer.writerow(scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a10b6",
   "metadata": {},
   "source": [
    "Webscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b86409",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def webscrape(date_range):\n",
    "    num_dates = len(date_range)\n",
    "    \n",
    "    print('\\nPronosoft Web-Scraping Application\\n')\n",
    "    print('\\nYou are accessing ...\\n')\n",
    "    print('   Website:  Pronosoft')\n",
    "    print('   URL:      http://www.pronosoft.com/fr/bookmakers/pronostics/\\n')\n",
    "    print('\\nProcessing & Scrapping ... ')\n",
    "    \n",
    "    prog = widgets.FloatProgress(min = 0, max = num_dates, bar_style = 'info')\n",
    "    prog.layout.margin = '24px'\n",
    "    display(prog)\n",
    "    \n",
    "    print('   ', end = '')\n",
    "\n",
    "    for count, date in enumerate(date_range):\n",
    "        website = 'http://www.pronosoft.com/fr/bookmakers/pronostics/' + date[:5] + '-20' + date[-2:] + '/'\n",
    "\n",
    "        page_soup = parse_page(website)\n",
    "        \n",
    "        if count == 0 or date[:2] == '01' or date[:2] == '15' or count == num_dates - 1:\n",
    "            print(date + ' | ', end = '')\n",
    "\n",
    "        day_scrape = get_info(page_soup, date)\n",
    "        save_info_csv(day_scrape)\n",
    "                \n",
    "        prog.value = count + 1\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eeefda",
   "metadata": {},
   "source": [
    "Get Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "385eed6c-2139-4257-98db-11ba05cd2244",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATE FORMAT: from Day Month Year (dmy) to Year Month Day (ymd) \n",
    "\n",
    "def date_format_revert(date):    \n",
    "    return '%s-%s-%s' % tuple(date.split('-')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52af1318",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATE RANGE: gives all dates between date_low and date_high\n",
    "\n",
    "def get_date_range(start_date, end_date):\n",
    "    pd_dates = pd.date_range(start = date_format_revert(start_date), end = date_format_revert(end_date))\n",
    "    date_range = [date_format_revert(str(date.date())) for date in pd_dates]     \n",
    "    \n",
    "    return date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0cdc35-4e2a-4c7e-9ed9-9ca4b95ad63c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Get Last Scrapped Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0715844-1d19-46ce-a80c-3e368f6e8f62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_last_scrape_date(file_csv):\n",
    "    try:\n",
    "        df = pd.read_csv(file_csv)\n",
    "        most_recent_date = list(df['date'])[-1]\n",
    "        print('\\nLast scrape on %s' % most_recent_date, '\\n')\n",
    "        \n",
    "        return most_recent_date\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print('\\nPronosoft data not yet scrapped\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bfc92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1c7da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e3a16",
   "metadata": {},
   "source": [
    "# Web-Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69dce828-afc9-488c-b252-893e63b63652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last scrape on 22-08-2021 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_recent_date = get_last_scrape_date(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e1d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pronosoft Web-Scraping Application\n",
      "\n",
      "\n",
      "You are accessing ...\n",
      "\n",
      "   Website:  Pronosoft\n",
      "   URL:      http://www.pronosoft.com/fr/bookmakers/pronostics/\n",
      "\n",
      "\n",
      "Processing & Scrapping ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a8f594b3f84638889e02a9df01580b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', layout=Layout(margin='24px'), max=4.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19-08-2021 | 22-08-2021 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Web Scrape range of dates | First possible date : 01-10-2018 \n",
    "\n",
    "start_date = '19-08-2021'\n",
    "end_date = '22-08-2021'\n",
    "\n",
    "dates = get_date_range(start_date, end_date)\n",
    "\n",
    "webscrape(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3800b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3f700-e09a-4ea7-9677-f5dc8b194482",
   "metadata": {},
   "source": [
    "# Web-Scrape | with widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c357edb-70f3-456d-818f-bfdaf8a06002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, please enter a range of dates to webscrape:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00ad7d1a538402b9d1c1adbf5caaea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DatePicker(value=None, description='Start Date'), DatePicker(value=None, description='End Date'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9a6445939e4627879880ea04348fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Continue', style=ButtonStyle(button_color='mediumaquamarine'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34ac7c7a6024c738e0c464e6581c3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output = widgets.Output()\n",
    "\n",
    "# @output.capture(clear_output = False, wait = True) \n",
    "# def sayHello(b):\n",
    "#     start_date_str = date_format_revert(str(start_date.value))\n",
    "#     end_date_str = date_format_revert(str(end_date.value))\n",
    "    \n",
    "#     dates = get_date_range(start_date_str, end_date_str)\n",
    "    \n",
    "#     print('\\n')\n",
    "#     webscrape(dates)\n",
    "    \n",
    "    \n",
    "\n",
    "# print(\"\\nHello, please enter a range of dates to webscrape:\\n\\n\")\n",
    "\n",
    "# start_date = widgets.DatePicker(description='Start Date', disabled=False)\n",
    "# end_date = widgets.DatePicker(description='End Date', disabled=False)\n",
    "# display(widgets.HBox([start_date, end_date]))\n",
    "\n",
    "# print('\\n')\n",
    "# run_button = widgets.Button(description = 'Continue')\n",
    "# run_button.style.button_color = 'mediumaquamarine'\n",
    "# run_button.on_click(sayHello)\n",
    "# display(run_button)\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2fd86-63ba-4c5e-9193-75d8228e0e4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b1e17-beda-4c8f-b1df-2c5cf6fbe6e8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
